#!/usr/bin/env python3
"""
çº¯è‡ªæˆ‘å¯¹å¼ˆè®­ç»ƒå™¨ - æ— ä»»ä½•ä¸“å®¶çŸ¥è¯†æ³¨å…¥
è§£å†³å›é€€é—®é¢˜çš„é€šç”¨æ–¹æ¡ˆ
"""

import random
import numpy as np
import torch
import torch.nn.functional as F
from torch.optim import Adam
from torch.optim.lr_scheduler import StepLR
import os
import json

from game import Game
from model import PolicyValueNet
from mcts import MCTS

class PureSelfPlayTrainer:
    """çº¯è‡ªæˆ‘å¯¹å¼ˆè®­ç»ƒå™¨ - æ— ä¸“å®¶ç³»ç»Ÿ"""
    
    def __init__(self):
        # é…ç½®å‚æ•°
        self.board_size = 3
        self.input_channels = 64      # ç¬¬ä¸€å±‚å·ç§¯é€šé“æ•°ï¼ˆæ¢å¤æˆåŠŸé…ç½®ï¼‰
        self.res_channels = [32, 64]  # æ®‹å·®å—é€šé“æ•°æ•°ç»„ï¼ˆæ¢å¤æˆåŠŸé…ç½®ï¼‰
        self.dropout_rate = 0.0
        
        # è®­ç»ƒè¶…å‚æ•° - å…³é”®ï¼šç¨³å®šçš„é…ç½®
        self.training_rounds = 1000
        self.mcts_simulations = 100
        self.learning_rate = 0.002  # åˆå§‹å­¦ä¹ ç‡
        self.lr_decay_step = 100    # å­¦ä¹ ç‡è¡°å‡æ­¥é•¿
        self.lr_decay_gamma = 0.9   # å­¦ä¹ ç‡è¡°å‡å› å­
        self.batch_size = 64        # æ›´å¤§æ‰¹æ¬¡ï¼Œç¨³å®šæ¢¯åº¦
        self.buffer_size = 2000     # æ›´å¤§ç¼“å†²åŒºï¼Œä¿æŒå¤šæ ·æ€§
        
        # ğŸ”‘ å…³é”®æ”¹è¿›ï¼šåŠ¨æ€æ¸©åº¦å’Œæ¢ç´¢ç­–ç•¥
        self.initial_temperature = 2.0  # é«˜åˆå§‹æ¸©åº¦ï¼Œå¼ºåˆ¶æ¢ç´¢
        self.final_temperature = 0.1    # ä½æœ€ç»ˆæ¸©åº¦ï¼Œæ”¶æ•›ç­–ç•¥
        self.temperature_decay_rounds = 600  # æ¸©åº¦è¡°å‡è½®æ•°
        
        # ğŸ”‘ å…³é”®æ”¹è¿›ï¼šæ¸è¿›å¼è®­ç»ƒå¼ºåº¦
        self.initial_games_per_round = 2    # åˆæœŸå°‘é‡æ¸¸æˆ
        self.final_games_per_round = 8      # åæœŸæ›´å¤šæ¸¸æˆ
        self.game_ramp_rounds = 300         # æ¸¸æˆæ•°é‡çˆ¬å¡è½®æ•°
        
        # ğŸ”‘ å…³é”®æ”¹è¿›ï¼šè‡ªé€‚åº”MCTSå‚æ•°
        self.initial_c_puct = 2.0   # é«˜æ¢ç´¢
        self.final_c_puct = 1.0     # æ ‡å‡†æ¢ç´¢
        
        # ç›‘æ§å’Œæ—©åœ
        self.evaluation_interval = 50
        self.patience = 200  # æ›´é•¿è€å¿ƒï¼Œé¿å…è¿‡æ—©åœæ­¢
        self.min_improvement = 0.01  # æœ€å°æ”¹å–„é˜ˆå€¼
        
        # æ€§èƒ½è¿½è¸ª
        self.performance_history = []
        
    def get_dynamic_temperature(self, round_num):
        """åŠ¨æ€æ¸©åº¦è°ƒåº¦"""
        if round_num <= self.temperature_decay_rounds:
            progress = round_num / self.temperature_decay_rounds
            temp = self.initial_temperature * (1 - progress) + self.final_temperature * progress
        else:
            temp = self.final_temperature
        return max(temp, 0.1)  # æœ€å°æ¸©åº¦ä¿æŠ¤
    
    def get_dynamic_games_count(self, round_num):
        """åŠ¨æ€æ¸¸æˆæ•°é‡"""
        if round_num <= self.game_ramp_rounds:
            progress = round_num / self.game_ramp_rounds
            games = int(self.initial_games_per_round * (1 - progress) + 
                       self.final_games_per_round * progress)
        else:
            games = self.final_games_per_round
        return max(games, 2)  # è‡³å°‘2å±€æ¸¸æˆ
    
    def get_dynamic_c_puct(self, round_num):
        """åŠ¨æ€æ¢ç´¢å‚æ•° - å¹³æ»‘è¡°å‡ï¼Œé¿å…çªç„¶ä¸‹é™"""
        # ğŸ”‘ å…³é”®ä¿®å¤ï¼šå»¶é•¿è¡°å‡æœŸï¼Œé¿å…è½®æ¬¡300çš„çªç„¶ä¸‹é™
        decay_rounds = 600  # å»¶é•¿åˆ°600è½®ï¼Œä¸æ¸©åº¦è¡°å‡ä¸€è‡´
        if round_num <= decay_rounds:
            progress = round_num / decay_rounds
            # ä½¿ç”¨å¹³æ»‘çš„æŒ‡æ•°è¡°å‡è€Œéçº¿æ€§è¡°å‡
            c_puct = self.initial_c_puct * (self.final_c_puct / self.initial_c_puct) ** progress
        else:
            c_puct = self.final_c_puct
        return max(c_puct, 1.0)  # ç¡®ä¿æœ€å°å€¼ä¸º1.0
    
    def evaluate_model(self, net, round_num):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½ - çº¯ç²¹åŸºäºç­–ç•¥åˆ†æ"""
        game = Game()
        
        # ä½¿ç”¨æ›´é«˜æ¸©åº¦è¯„ä¼°ï¼Œé¿å…MCTSè¿‡åº¦æ”¾å¤§ç½‘ç»œåå¥½
        mcts = MCTS(net, c_puct=1.0, n_playout=100)
        moves, probs = mcts.get_move_probs(game, temp=1.0)  # ä¿®å¤ï¼šä½¿ç”¨æ¸©åº¦1.0
        
        # è®¡ç®—ä¸­å¿ƒåå¥½
        center_prob = 0
        corner_probs = []
        edge_probs = []
        
        for move, prob in zip(moves, probs):
            if move == (1, 1):  # ä¸­å¿ƒ
                center_prob = prob
            elif move in [(0, 0), (0, 2), (2, 0), (2, 2)]:  # è§’è½
                corner_probs.append(prob)
            else:  # è¾¹ç¼˜
                edge_probs.append(prob)
        
        # ä¿®å¤ï¼šé¿å…æå°æ¦‚ç‡å¯¼è‡´çš„å¤©æ–‡æ•°å­—æ¯”å€¼
        avg_corner = max(np.mean(corner_probs) if corner_probs else 0.0001, 1e-6)
        avg_edge = max(np.mean(edge_probs) if edge_probs else 0.0001, 1e-6)
        
        # ä½¿ç”¨å®‰å…¨çš„æ¯”å€¼è®¡ç®—ï¼Œé¿å…é™¤é›¶å’Œå¤©æ–‡æ•°å­—
        center_corner_ratio = min(center_prob / avg_corner, 1000.0)  # é™åˆ¶æœ€å¤§æ¯”å€¼
        center_edge_ratio = min(center_prob / avg_edge, 1000.0)      # é™åˆ¶æœ€å¤§æ¯”å€¼
        
        # ç»¼åˆè¯„åˆ†ï¼šæ—¢è¦ä¸­å¿ƒåå¥½ï¼Œä¹Ÿè¦åˆç†çš„è¾¹ç¼˜ç­–ç•¥
        # ç†æƒ³çš„äº•å­—æ£‹ç­–ç•¥ï¼šä¸­å¿ƒ > è§’è½ > è¾¹ç¼˜
        ideal_score = center_corner_ratio * 0.7 + center_edge_ratio * 0.3
        
        return {
            'center_prob': center_prob,
            'corner_prob': avg_corner,
            'edge_prob': avg_edge,
            'center_corner_ratio': center_corner_ratio,
            'center_edge_ratio': center_edge_ratio,
            'ideal_score': ideal_score,
            'round': round_num
        }
    
    def self_play_game(self, net, round_num):
        """å•å±€è‡ªæˆ‘å¯¹å¼ˆ"""
        game = Game()
        
        # åŠ¨æ€å‚æ•°
        temperature = self.get_dynamic_temperature(round_num)
        c_puct = self.get_dynamic_c_puct(round_num)
        
        mcts = MCTS(net, c_puct=c_puct, n_playout=self.mcts_simulations)
        
        states, probs, rewards = [], [], []
        
        # æ¸¸æˆä¸»å¾ªç¯
        winner = None
        while True:
            if game.is_full():
                winner = 0  # å¹³å±€
                break
            
            # è·å–MCTSç­–ç•¥
            state = game.get_state()
            moves, move_probs = mcts.get_move_probs(game, temp=temperature)
            
            # è®°å½•è®­ç»ƒæ•°æ®
            states.append(state.copy())
            prob_array = np.zeros(9)
            for move, prob in zip(moves, move_probs):
                move_idx = move[0] * 3 + move[1] if isinstance(move, tuple) else move
                prob_array[move_idx] = prob
            probs.append(prob_array)
            
            # é€‰æ‹©å¹¶æ‰§è¡ŒåŠ¨ä½œ
            action_idx = np.random.choice(len(moves), p=move_probs)
            selected_move = moves[action_idx]
            
            is_win = game.make_move(selected_move)
            if is_win:
                winner = 2 if game.current_player == 1 else 1
                break
            
            # æ›´æ–°MCTSçŠ¶æ€
            update_move = (selected_move[0] * 3 + selected_move[1] 
                          if isinstance(selected_move, tuple) else selected_move)
            mcts.update_with_move(update_move)
        
        # è®¡ç®—æ¯ä¸ªä½ç½®çš„å¥–åŠ±
        game_data = []
        for i, (state, prob) in enumerate(zip(states, probs)):
            current_player = 1 if i % 2 == 0 else -1
            if winner == 0:
                reward = 0.0  # å¹³å±€
            elif winner == current_player:
                reward = 1.0  # è·èƒœ
            else:
                reward = -1.0  # å¤±è´¥
            
            game_data.append((state, prob, reward))
        
        return game_data
    
    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        print("ğŸ¯ çº¯è‡ªæˆ‘å¯¹å¼ˆè®­ç»ƒå™¨ - æ— ä¸“å®¶çŸ¥è¯†")
        print("="*60)
        print(f"ğŸ§  ç½‘ç»œæ¶æ„: input_channels={self.input_channels}, res_channels={self.res_channels}")
        print(f"ğŸ” MCTSæœç´¢: {self.mcts_simulations}æ¬¡")
        print(f"ğŸŒ¡ï¸ æ¸©åº¦èŒƒå›´: {self.initial_temperature} â†’ {self.final_temperature}")
        print(f"ğŸ® æ¸¸æˆæ•°é‡: {self.initial_games_per_round} â†’ {self.final_games_per_round}")
        print(f"âš–ï¸ æ¢ç´¢å‚æ•°: {self.initial_c_puct} â†’ {self.final_c_puct}")
        print(f"ğŸ’¾ ç¼“å†²åŒºå¤§å°: {self.buffer_size}")
        print("")
        
        # åˆå§‹åŒ–ç½‘ç»œå’Œä¼˜åŒ–å™¨
        net = PolicyValueNet(
            board_size=self.board_size,
            input_channels=self.input_channels,
            res_channels=self.res_channels,
            dropout_rate=self.dropout_rate
        )
        
        optimizer = Adam(net.parameters(), lr=self.learning_rate, weight_decay=1e-4)
        # ğŸ”‘ å…³é”®ä¿®å¤ï¼šæ·»åŠ å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼Œé˜²æ­¢åæœŸè¿‡æ‹Ÿåˆ
        scheduler = StepLR(optimizer, step_size=self.lr_decay_step, gamma=self.lr_decay_gamma)
        replay_buffer = []
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs("pure_selfplay_checkpoint", exist_ok=True)
        
        # è®­ç»ƒçŠ¶æ€è¿½è¸ª
        best_score = 0
        best_round = 0
        no_improvement_count = 0
        
        # ä¸»è®­ç»ƒå¾ªç¯
        for round_num in range(1, self.training_rounds + 1):
            current_temp = self.get_dynamic_temperature(round_num)
            current_games = self.get_dynamic_games_count(round_num)
            current_c_puct = self.get_dynamic_c_puct(round_num)
            current_lr = optimizer.param_groups[0]['lr']
            
            print(f"è½®æ¬¡ {round_num}/{self.training_rounds} "
                  f"(T={current_temp:.2f}, æ¸¸æˆ={current_games}, C={current_c_puct:.2f}, LR={current_lr:.5f})")
            
            # 1. è‡ªæˆ‘å¯¹å¼ˆç”Ÿæˆæ•°æ®
            round_data = []
            for game_idx in range(current_games):
                game_data = self.self_play_game(net, round_num)
                round_data.extend(game_data)
            
            # 2. æ›´æ–°å›æ”¾ç¼“å†²åŒº
            replay_buffer.extend(round_data)
            if len(replay_buffer) > self.buffer_size:
                # ğŸ”‘ å…³é”®ï¼šä¿æŒç¼“å†²åŒºå¤šæ ·æ€§ï¼Œéšæœºç§»é™¤è€ŒéFIFO
                excess = len(replay_buffer) - self.buffer_size
                indices_to_remove = random.sample(range(len(replay_buffer)), excess)
                replay_buffer = [replay_buffer[i] for i in range(len(replay_buffer)) 
                               if i not in indices_to_remove]
            
            # 3. ç½‘ç»œè®­ç»ƒ
            if len(replay_buffer) >= self.batch_size:
                # å¤šæ¬¡è®­ç»ƒä»¥æé«˜å­¦ä¹ æ•ˆç‡
                train_iterations = max(1, len(round_data) // 16)  # è‡ªé€‚åº”è®­ç»ƒæ¬¡æ•°
                
                for _ in range(train_iterations):
                    batch = random.sample(replay_buffer, self.batch_size)
                    
                    states = torch.FloatTensor([item[0] for item in batch]).to(net.device)
                    target_probs = torch.FloatTensor([item[1] for item in batch]).to(net.device)
                    target_values = torch.FloatTensor([item[2] for item in batch]).to(net.device)
                    
                    # å‰å‘ä¼ æ’­
                    log_probs, values = net(states)
                    
                    # æŸå¤±è®¡ç®—
                    value_loss = F.mse_loss(values.squeeze(), target_values)
                    policy_loss = -torch.mean(torch.sum(target_probs * log_probs, dim=1))
                    total_loss = value_loss + policy_loss
                    
                    # åå‘ä¼ æ’­
                    optimizer.zero_grad()
                    total_loss.backward()
                    # æ¢¯åº¦è£å‰ªï¼Œç¨³å®šè®­ç»ƒ
                    torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)
                    optimizer.step()
            
            # 4. å®šæœŸè¯„ä¼°
            if round_num % self.evaluation_interval == 0:
                performance = self.evaluate_model(net, round_num)
                self.performance_history.append(performance)
                
                print(f"  ğŸ“Š è¯„ä¼°ç»“æœ:")
                print(f"    ä¸­å¿ƒæ¦‚ç‡: {performance['center_prob']:.4f}")
                print(f"    ä¸­å¿ƒ/è§’è½æ¯”å€¼: {performance['center_corner_ratio']:.3f}")
                print(f"    ç»¼åˆè¯„åˆ†: {performance['ideal_score']:.3f}")
                
                # æ—©åœæ£€æŸ¥
                if performance['ideal_score'] > best_score + self.min_improvement:
                    best_score = performance['ideal_score']
                    best_round = round_num
                    no_improvement_count = 0
                    
                    # ä¿å­˜æœ€ä½³æ¨¡å‹
                    best_model_path = "pure_selfplay_checkpoint/best_model.pth"
                    torch.save({
                        'model_state_dict': net.state_dict(),
                        'round': round_num,
                        'performance': performance,
                        'board_size': self.board_size,
                        'input_channels': self.input_channels,
                        'res_channels': self.res_channels,
                        'dropout_rate': self.dropout_rate,
                        'training_config': {
                            'temperature_range': (self.initial_temperature, self.final_temperature),
                            'c_puct_range': (self.initial_c_puct, self.final_c_puct)
                        }
                    }, best_model_path)
                    print(f"    ğŸ† æ–°æœ€ä½³æ¨¡å‹! è¯„åˆ†: {best_score:.3f}")
                else:
                    no_improvement_count += self.evaluation_interval
                
                # å®šæœŸä¿å­˜
                if round_num % (self.evaluation_interval * 4) == 0:
                    model_path = f"pure_selfplay_checkpoint/model_{round_num}.pth"
                    torch.save({
                        'model_state_dict': net.state_dict(),
                        'round': round_num,
                        'performance': performance
                    }, model_path)
                    print(f"    ğŸ’¾ å·²ä¿å­˜æ£€æŸ¥ç‚¹: {model_path}")
                
                # æ—©åœæ£€æŸ¥
                if no_improvement_count >= self.patience:
                    print(f"  â¹ï¸ æ—©åœ: {self.patience}è½®æ— æ˜¾è‘—æ”¹å–„")
                    break
            
            # ğŸ”‘ å…³é”®ä¿®å¤ï¼šå­¦ä¹ ç‡è°ƒåº¦ï¼Œé˜²æ­¢åæœŸè¿‡æ‹Ÿåˆ
            scheduler.step()
        
        # ä¿å­˜è®­ç»ƒå†å²
        history_path = "pure_selfplay_checkpoint/training_history.json"
        with open(history_path, 'w') as f:
            json.dump(self.performance_history, f, indent=2)
        
        print(f"âœ… çº¯è‡ªæˆ‘å¯¹å¼ˆè®­ç»ƒå®Œæˆ!")
        print(f"ğŸ† æœ€ä½³æ¨¡å‹: è½®æ¬¡{best_round}, è¯„åˆ†{best_score:.3f}")
        print(f"ğŸ“Š è®­ç»ƒå†å²å·²ä¿å­˜åˆ°: {history_path}")

if __name__ == "__main__":
    trainer = PureSelfPlayTrainer()
    trainer.train()
