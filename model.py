import torch
import torch.nn as nn
import torch.nn.functional as F

class ResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):
        super(ResidualBlock, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size, 1, padding)
        self.bn2 = nn.BatchNorm2d(out_channels)

        #  shortcutè¿æ¥ï¼Œå¦‚æœè¾“å…¥è¾“å‡ºé€šé“æ•°ä¸åŒï¼Œåˆ™ä½¿ç”¨1x1å·ç§¯è°ƒæ•´
        if in_channels != out_channels or stride != 1:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),
                nn.BatchNorm2d(out_channels)
            )
        else:
            self.shortcut = nn.Identity()

    def forward(self, x):
        residual = self.shortcut(x)
        x = F.relu(self.bn1(self.conv1(x)))
        x = self.bn2(self.conv2(x))
        x += residual
        x = F.relu(x)
        return x

class PolicyValueNet(nn.Module):
    def __init__(self, board_size=3, input_channels=64, res_channels=[32, 64], dropout_rate=0.3):
        super(PolicyValueNet, self).__init__()
        self.board_size = board_size
        self.input_channels = input_channels  # ç¬¬ä¸€å±‚å·ç§¯çš„é€šé“æ•°
        self.res_channels = res_channels     # æ®‹å·®å—çš„é€šé“æ•°æ•°ç»„
        self.dropout_rate = dropout_rate
        
        # è¾“å…¥å±‚ï¼š3ä¸ªé€šé“ï¼ˆæ£‹ç›˜çŠ¶æ€ + å½“å‰ç©å®¶æ ‡è®° + å¸¸æ•°å±‚ï¼‰â†’ input_channels
        self.input_conv = nn.Conv2d(3, input_channels, kernel_size=3, padding=1)
        self.input_bn = nn.BatchNorm2d(input_channels)
        
        # æ®‹å·®å—ï¼šä½¿ç”¨res_channelsæ•°ç»„é…ç½®
        self.res_blocks = nn.ModuleList()
        in_channels = input_channels  # ç¬¬ä¸€ä¸ªæ®‹å·®å—çš„è¾“å…¥æ¥è‡ªinput_conv
        for out_channels in res_channels:
            self.res_blocks.append(ResidualBlock(in_channels, out_channels))
            in_channels = out_channels  # ä¸‹ä¸€ä¸ªå—çš„è¾“å…¥é€šé“æ•°
        
        # æœ€åä¸€ä¸ªæ®‹å·®å—çš„è¾“å‡ºé€šé“æ•°
        final_channels = res_channels[-1] if res_channels else input_channels
        
        # ç­–ç•¥å¤´ - å‚è€ƒæˆåŠŸå®ç°çš„è®¾è®¡
        policy_channels = max(final_channels // 4, 8)
        self.policy_conv = nn.Conv2d(final_channels, policy_channels, kernel_size=1)
        self.policy_bn = nn.BatchNorm2d(policy_channels)
        self.policy_dropout = nn.Dropout(dropout_rate)
        self.policy_fc = nn.Linear(policy_channels * board_size * board_size, board_size * board_size)

        # ä»·å€¼å¤´ - å‚è€ƒæˆåŠŸå®ç°çš„è®¾è®¡
        value_channels = max(final_channels // 4, 8)
        self.value_conv = nn.Conv2d(final_channels, value_channels, kernel_size=1)
        self.value_bn = nn.BatchNorm2d(value_channels)
        
        # ç®€å•çš„2å±‚ç»“æ„ï¼Œé€‚åˆäº•å­—æ£‹
        value_input_size = value_channels * board_size * board_size
        value_hidden_size = max(final_channels // 2, 32)
        self.value_dropout = nn.Dropout(dropout_rate)
        self.value_fc1 = nn.Linear(value_input_size, value_hidden_size)
        self.value_fc2 = nn.Linear(value_hidden_size, 1)

        # æ™ºèƒ½è®¾å¤‡é€‰æ‹©ï¼šæ ¹æ®ç½‘ç»œè§„æ¨¡å’Œç¡¬ä»¶ç‰¹æ€§é€‰æ‹©æœ€ä¼˜è®¾å¤‡
        self.device = self._select_optimal_device()
        self.to(self.device)
    
    def _select_optimal_device(self):
        """æ™ºèƒ½é€‰æ‹©æœ€ä¼˜è®¡ç®—è®¾å¤‡"""
        if not torch.cuda.is_available():
            return torch.device("cpu")
        
        # è®¡ç®—ç½‘ç»œå‚æ•°æ€»æ•°
        total_params = sum(p.numel() for p in self.parameters())
        
        # å°ç½‘ç»œ(<1Må‚æ•°)åœ¨CPUä¸Šå¯èƒ½æ›´é«˜æ•ˆï¼Œé¿å…GPUå¼€é”€
        if total_params < 1_000_000:
            # å¯¹äºäº•å­—æ£‹è¿™ç§å°ç½‘ç»œï¼Œæµ‹è¯•GPU vs CPUæ€§èƒ½
            try:
                # å¿«é€ŸåŸºå‡†æµ‹è¯•
                import time
                test_input = torch.randn(1, 3, 3, 3)
                
                # æµ‹è¯•CPU
                self.to('cpu')
                test_input_cpu = test_input.to('cpu')
                start = time.time()
                with torch.no_grad():
                    for _ in range(10):
                        _ = self.forward(test_input_cpu)
                cpu_time = time.time() - start
                
                # æµ‹è¯•GPU
                self.to('cuda')
                test_input_gpu = test_input.to('cuda')
                torch.cuda.synchronize()
                start = time.time()
                with torch.no_grad():
                    for _ in range(10):
                        _ = self.forward(test_input_gpu)
                    torch.cuda.synchronize()
                gpu_time = time.time() - start
                
                # é€‰æ‹©æ›´å¿«çš„è®¾å¤‡ï¼Œå¹¶æ·»åŠ å®‰å…¨è¾¹é™…
                if cpu_time * 1.2 < gpu_time:  # CPUéœ€è¦æ˜æ˜¾æ›´å¿«æ‰é€‰æ‹©
                    selected_device = torch.device("cpu")
                    reason = f"å°ç½‘ç»œCPUæ›´ä¼˜(CPU:{cpu_time:.4f}s vs GPU:{gpu_time:.4f}s)"
                else:
                    selected_device = torch.device("cuda")
                    reason = f"GPUä»æœ‰ä¼˜åŠ¿(CPU:{cpu_time:.4f}s vs GPU:{gpu_time:.4f}s)"
                    
                print(f"ğŸ¯ æ™ºèƒ½è®¾å¤‡é€‰æ‹©: {selected_device} - {reason}")
                return selected_device
                
            except Exception as e:
                print(f"âš ï¸ è®¾å¤‡åŸºå‡†æµ‹è¯•å¤±è´¥ï¼Œä½¿ç”¨CPU: {e}")
                return torch.device("cpu")
        else:
            # å¤§ç½‘ç»œä¼˜å…ˆä½¿ç”¨GPU
            print(f"ğŸš€ å¤§ç½‘ç»œ({total_params:,}å‚æ•°)ä½¿ç”¨GPUåŠ é€Ÿ")
            return torch.device("cuda")

    def forward(self, state):
        # è¾“å…¥å±‚
        x = F.relu(self.input_bn(self.input_conv(state)))
        
        # é€šè¿‡æ®‹å·®å—
        for block in self.res_blocks:
            x = block(x)
        
        # ç­–ç•¥å¤´ - åŠ å…¥dropouté˜²æ­¢è¿‡æ‹Ÿåˆ
        policy = F.relu(self.policy_bn(self.policy_conv(x)))
        policy = policy.view(-1, self.policy_conv.out_channels * self.board_size * self.board_size)
        policy = self.policy_dropout(policy)
        policy = F.log_softmax(self.policy_fc(policy), dim=1)

        # ä»·å€¼å¤´ - åŠ å…¥dropouté˜²æ­¢è¿‡æ‹Ÿåˆ
        value = F.relu(self.value_bn(self.value_conv(x)))
        value = value.view(-1, self.value_conv.out_channels * self.board_size * self.board_size)
        value = self.value_dropout(value)
        value = F.relu(self.value_fc1(value))
        value = torch.tanh(self.value_fc2(value))

        return policy, value

    def predict(self, state):
        state = torch.FloatTensor(state).to(self.device)
        with torch.no_grad():
            policy, value = self.forward(state)
        return policy.cpu().numpy(), value.cpu().numpy()